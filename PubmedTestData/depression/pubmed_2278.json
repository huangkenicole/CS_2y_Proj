{
  "CitationSubset": [
    "IM"
  ],
  "KeywordList": [
    [
      "Biomedical informatics",
      "Biomedical information processing",
      "Deep learning",
      "Features fusion",
      "Multimodal depression recognition"
    ]
  ],
  "GeneralNote": [],
  "OtherAbstract": [],
  "SpaceFlightMission": [],
  "OtherID": [],
  "InvestigatorList": [],
  "PMID": "34614452",
  "DateCompleted": {
    "Year": "2021",
    "Month": "10",
    "Day": "28"
  },
  "DateRevised": {
    "Year": "2022",
    "Month": "05",
    "Day": "31"
  },
  "Article": {
    "ArticleDate": [
      {
        "Year": "2021",
        "Month": "09",
        "Day": "28"
      }
    ],
    "Language": [
      "eng"
    ],
    "ELocationID": [
      "10.1016/j.cmpb.2021.106433",
      "S0169-2607(21)00507-1"
    ],
    "Journal": {
      "ISSN": "1872-7565",
      "JournalIssue": {
        "Volume": "211",
        "PubDate": {
          "Year": "2021",
          "Month": "Nov"
        }
      },
      "Title": "Computer methods and programs in biomedicine",
      "ISOAbbreviation": "Comput Methods Programs Biomed"
    },
    "ArticleTitle": "End-to-end multimodal clinical depression recognition using deep neural networks: A comparative analysis.",
    "Pagination": {
      "StartPage": "106433",
      "MedlinePgn": "106433"
    },
    "Abstract": {
      "AbstractText": [
        "Major Depressive Disorder is a highly prevalent and disabling mental health condition. Numerous studies explored multimodal fusion systems combining visual, audio, and textual features via deep learning architectures for clinical depression recognition. Yet, no comparative analysis for multimodal depression analysis has been proposed in the literature.",
        "In this paper, an up-to-date literature overview of multimodal depression recognition is presented and an extensive comparative analysis of different deep learning architectures for depression recognition is performed. First, audio features based Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) are studied. Then, early-level and model-level fusion of deep audio features with visual and textual features through LSTM and CNN architectures are investigated.",
        "The performance of the proposed architectures using an hold-out strategy on the DAIC-WOZ dataset (80% training, 10% validation, 10% test split) for binary and severity levels of depression recognition is tested. Using this strategy, a set of experiments have been performed and they have demonstrated: (1) LSTM-based audio features perform slightly better than CNN ones with an accuracy of 66.25% versus 65.60% for binary depression classes. (2) the model level fusion of deep audio and visual features using LSTM network performed the best with an accuracy of 77.16%, a precision of 53% for the depressed class, and a precision of 83% for the non-depressed class. The given network obtained a normalized Root Mean Square Error (RMSE) of 0.15 for depression severity level prediction. Using a Leave-One-Subject-Out strategy, this network achieved an accuracy of 95.38% for binary depression detection, and a normalized RMSE of 0.1476 for depression severity level prediction. Our best-performing architecture outperforms all state-of-the-art approaches on DAIC-WOZ dataset.",
        "The obtained results show that the proposed LSTM-based surpass the proposed CNN-based architectures allowing to learn temporal dynamics representations of multimodal features. Furthermore, model-level fusion of audio and visual features using an LSTM network leads to the best performance. Our best-performing architecture successfully detects depression using a speech segment of less than 8 seconds, and an average prediction computation time of less than 6ms; making it suitable for real-world clinical applications."
      ],
      "CopyrightInformation": "Copyright \u00a9 2021 Elsevier B.V. All rights reserved."
    },
    "AuthorList": [
      {
        "Identifier": [],
        "AffiliationInfo": [
          {
            "Identifier": [],
            "Affiliation": "Universit\u00e9 Paris-Est Cr\u00e9teil (UPEC), LISSI, Vitry sur Seine 94400, France."
          }
        ],
        "LastName": "Muzammel",
        "ForeName": "Muhammad",
        "Initials": "M"
      },
      {
        "Identifier": [],
        "AffiliationInfo": [
          {
            "Identifier": [],
            "Affiliation": "New York University, SMART Lab, Saadiyat Island, Abu Dhabi."
          }
        ],
        "LastName": "Salam",
        "ForeName": "Hanan",
        "Initials": "H"
      },
      {
        "Identifier": [],
        "AffiliationInfo": [
          {
            "Identifier": [],
            "Affiliation": "Universit\u00e9 Paris-Est Cr\u00e9teil (UPEC), LISSI, Vitry sur Seine 94400, France. Electronic address: alice.othmani@u-pec.fr."
          }
        ],
        "LastName": "Othmani",
        "ForeName": "Alice",
        "Initials": "A"
      }
    ],
    "PublicationTypeList": [
      "Comparative Study",
      "Journal Article"
    ]
  },
  "MedlineJournalInfo": {
    "Country": "Ireland",
    "MedlineTA": "Comput Methods Programs Biomed",
    "NlmUniqueID": "8506513",
    "ISSNLinking": "0169-2607"
  },
  "MeshHeadingList": [
    {
      "QualifierName": [
        "diagnosis"
      ],
      "DescriptorName": "Depression"
    },
    {
      "QualifierName": [
        "diagnosis"
      ],
      "DescriptorName": "Depressive Disorder, Major"
    },
    {
      "QualifierName": [],
      "DescriptorName": "Humans"
    },
    {
      "QualifierName": [],
      "DescriptorName": "Neural Networks, Computer"
    }
  ],
  "CoiStatement": "Declaration of Competing Interest The author(s) declare(s) that there is no conflict of interest."
}